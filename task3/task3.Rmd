---
title: "Assignment on blackbox models"
author: Igor Tresoumov, BBI 141
output:
  word_document: default
  html_notebook: default
---

Dr. Reddy's Laboratories has been chosen to complete the assignment.

The data is obtained from Google Finance for the stock and from Yahoo Finance for the S&P 500 index for a period since 2009/01/01, each observation is a single day.

**All statistical assumptions and conclusions are made on the 5% significance level.**

The data is obtained the following way:
```{r message=FALSE}
library(quantmod)

stock = as.numeric(na.omit(Cl(getSymbols('RDY', src='google', auto.assign=FALSE, from='2007-01-01'))))
log_rt = na.omit(diff(log(stock)))

sp5 = as.numeric(na.omit(Cl(getSymbols('^GSPC', src='yahoo', auto.assign=FALSE, from='2007-01-01'))))
sp_rt = na.omit(diff(log(sp5)))
```

## Task (a)

> Divide the data set into the training and forecast subsets. Clearly state this partition
in your report.

The feature vectors and, ultimately, the feature matrix will be prepared:
```{r}
n_obs = length(log_rt)
rt_feat = rep.int(0, n_obs)
rt_feat[log_rt>0] = 1

sp_feat = rep.int(0, n_obs)
sp_feat[sp_rt>0] = 1

feat_matrix = matrix(0, n_obs-3, 7)

feat_matrix[,1] = rt_feat[4:n_obs]
feat_matrix[,2] = rt_feat[3:(n_obs-1)]
feat_matrix[,3] = rt_feat[2:(n_obs-2)]
feat_matrix[,4] = rt_feat[1:(n_obs-3)]

feat_matrix[,5] = sp_feat[3:(n_obs-1)]
feat_matrix[,6] = sp_feat[2:(n_obs-2)]
feat_matrix[,7] = sp_feat[1:(n_obs-3)]
```

Then the feature matrix will be converted to a dataframe object, which then will be partitioned into two subsets in the ratio 70/30:
```{r}
features = as.data.frame(feat_matrix)
train = features[1:2000, ]
test = features[2001:2714, ]
```

## Task (b)

> Fit a linear logistic regression model for P(Dt = 1) using Dt-i, Mt-i, i = 1; 2; 3 as
explanatory variables. Use only the training subset for estimation. Discuss statistical
significance of the coefficients. Refine the model if needed.

```{r}
log_reg = glm(V1~V2+V3+V4+V5+V6+V7, data=train, family="binomial")
summary(log_reg)
```

As only the fifth variable (S&P 500 3 steps before) is statistically significant, the rest can be omitted.

```{r}
log_reg = glm(V1~V5, data=train, family="binomial")
summary(log_reg)
```

The new model's AIC has dropped, but not very much; in return one gets the benefit of a simpler model.

## Task (c)

> Using the model, make predictions for the forecast subset. Specify the threshold you
apply. Compute the forecast error.

First, the forecast for the test subset will be obtained:
```{r}
log_forecast = predict(log_reg, test, type="response")
```

Then it will be converted to binary variable and represented as a table:
```{r}
log_pred = rep.int(0, length(log_forecast))
log_pred[log_forecast>0.5] = 1
table(log_pred, test$V1)
```

```{r}
accuracy = (202+185)/(202+185+158+169)
accuracy
```

The accuracy rate is 0.54.

## Task (d)

> Apply the linear discriminant analysis instead of log regression in items 1a-1c. Compute
the forecast error.

In the following chunk an LDA model will be computed:
```{r}
library(MASS)

lda_model = lda(V1~V2+V3+V4+V5+V6+V7, data=train)
lda_model
```

```{r}
lda_forecast = predict(lda_model, test, type="response")$class
table(lda_forecast, test$V1)
```

```{r}
accuracy = (153+235)/(153+235+207+119)
accuracy
```

While the accuracy rate is higher, this growth cannot be considered dramatic: predictive power is roughly equal compared to the algorithm above.

## Task (f)

> Apply the quadratic discriminant analysis instead of log regression in items 1a-1c.
Compute the forecast error.

In the following line a QDA model will be applied to the data:
```{r}
qda_model = qda(V1~V2+V3+V4+V5+V6+V7, data=train)
qda_model
```

```{r}
qda_forecast = predict(qda_model, test, type="response")$class
table(qda_forecast, test$V1)
```

```{r}
accuracy = (117+227)/(117+227+127+243)
accuracy
```

The forecasting power of this model has dropped, only reaching 48%.

## Task (f)

> Employ a 4-3-1 look-forward neural network with direct link for P(Dt = 1) instead of
log regression in items 1a-1c. Build a model for two (i = 1; 2) lagged variables D, M.
Compute the forecast error.

```{r}
feat_matrix = matrix(0, n_obs-2, 5)

feat_matrix[,1] = rt_feat[3:n_obs]
feat_matrix[,2] = rt_feat[2:(n_obs-1)]
feat_matrix[,3] = rt_feat[1:(n_obs-2)]

feat_matrix[,4] = sp_feat[2:(n_obs-1)]
feat_matrix[,5] = sp_feat[1:(n_obs-2)]

features = as.data.frame(feat_matrix)
train = features[1:2000, ]
test = features[2001:2715, ]
```

```{r}
library(nnet)

nnet_model = nnet(V1~V2+V3+V4+V5, data=train, size=3)
summary(nnet_model)
```

```{r}
nnet_forecast = predict(nnet_model, test)

nnet_pred = rep.int(0, length(nnet_forecast))
nnet_pred[nnet_forecast>0.5] = 1
table(nnet_pred, test$V1)
```

```{r}
accuracy = (149+212)/(149+142+212+212)
accuracy
```

The predictive power of the neural network is roughly equivalent to a constant or throwing a coin: 50% chance of success.


## Task (g)

> Compare the predictive power (i.e, forecast error) of methods 1b,1d,1e,1f.

Ordering by accuracy rate results in the following:

1. Linear Discriminant Analysis

2. Logistic Regression

3. Neural Network

4. Quadratic Discriminant Analysis

However, their predictive power is at 50% level: one cannot rely on them to make decisions, other factors should be included.